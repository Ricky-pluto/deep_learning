# 然而模型参数的数量也会随之呈指数增长， 因为词表需要存储Vn个数字， 因此与其将模型化， 不如使用隐变量模型：

# 隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层， 而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的输入， 并且这些状态只能通过先前时间步的数据来计算。

# 循环神经网络（recurrent neural networks，RNNs） 是具有隐状态的神经网络

# 具体地说，当前时间步隐藏变量由当前时间步的输入 与前一个时间步的隐藏变量一起计算得出：

# 这些变量捕获并保留了序列直到其当前时间步的历史信息， 就如当前时间步下神经网络的状态或记忆， 因此这样的隐藏变量被称为隐状态（hidden state）。 由于在当前时间步中， 隐状态使用的定义与前一个时间步中使用的定义相同， 因此 (8.4.5)的计算是循环的（recurrent）。 于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。

# 循环神经网络的参数包括隐藏层的权重 和偏置， 以及输出层的权重和偏置

# 拼接当前时间步的输入和前一时间步的隐状态；
# 将拼接的结果送入带有激活函数的全连接层。 全连接层的输出是当前时间步的隐状态

# 因此，它应该允许我们在压缩序列时花费更少的比特。 所以我们可以通过一个序列中所有的个词元的交叉熵损失的平均值来衡量：
# 其中由语言模型给出， 是在时间步从该序列中观察到的实际词元。 这使得不同长度的文档的性能具有了可比性。 由于历史原因，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。 简而言之，它是 (8.4.7)的指数

